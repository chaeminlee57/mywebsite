function typeWriter(text, elementId, speed = 100, startDelay = 0) {
    const element = document.getElementById(elementId);
    element.innerHTML = '';
    let i = 0;
    
    function type() {
        if (i < text.length) {
            element.innerHTML += text.charAt(i);
            i++;
            setTimeout(type, speed);
        }
    }
    setTimeout(type, startDelay);
}

function fadeIn(elementId, delay) {
    setTimeout(() => {
        const element = document.getElementById(elementId);
        element.classList.add('fadeIn');
    }, delay);
}



typeWriter("CHAEMIN LEE.", "typing-name", 80, 0);
fadeIn("university", 1000);
typeWriter("PROJECTS", "typing-project", 80, 1000);
typeWriter("Synthetic Fraudulent Data Generator", "typing-project1-name", 30, 1000);
typeWriter("As AI models grow larger and larger, it starts running out of data to train itself on. In this case, models would start resorting to training on synthetic data generated by other models or a library like Faker in some cases. This, however, poses some technical challenge, as synthetic data, particularly for trend-sensitive fields like fraudulent transactions, would have to constantly re-adjust itself to align with real-world examples. There's also the need to account for edge-case creation, which would be much harder with GANs (Generative Adversarial Networks) than just using a preexisting example. My project aims to synthesize fraudulent data so that fraudulent detection models can improve its accuracy. I've included the baseline models for some of the technical challenges, such as automatically readjusting its training batch depending on trends and accounting edge-case scenarios to a tasteful degree.", "typing-project1-desc", 3, 1000);
